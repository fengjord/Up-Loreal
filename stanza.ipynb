{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.protobuf.internal.containers' has no attribute 'MutableMapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-50e0f9192292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultilingual\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultilingualPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstallation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minstall_corenlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_corenlp_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepparse_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDepparseProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstituency_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConstituencyProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mner_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDEFAULT_MODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\pipeline\\constituency_processor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \"\"\"\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstituency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstituency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_transitions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransitionScheme\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstituency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mretag_trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_eval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluateParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\server\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexedWord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParseTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDependencyGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCorefChain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNERMention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRelation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRelationTriple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQuote\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpeakerInfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stanza\\protobuf\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_EncodeVarint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_DecodeVarint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwire_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\wire_format.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m   \u001b[1;32mimport\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m   \u001b[0m_USE_C_DESCRIPTORS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.protobuf.internal.containers' has no attribute 'MutableMapping'"
     ]
    }
   ],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('garnierEnComBV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename(columns={'Review':'review_text'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline()\n",
    "\n",
    "def preprocess(review):\n",
    "      sentence = review\n",
    "      sentence = re.sub('[^a-zA-Z]',' ',sentence)\n",
    "      sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ',sentence)\n",
    "      sentence = re.sub(r'\\s+', ' ',sentence)\n",
    "\n",
    "      return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = []\n",
    "for i in range(len(df)):\n",
    "    txt = df['review_text'][i]\n",
    "    txt=txt.lower()\n",
    "\n",
    "    #Create a def to remove all the upper lower case, punctuations, and extra white space\n",
    "    \n",
    "    def preprocess(review):\n",
    "      sentence = review\n",
    "      sentence = re.sub('[^a-zA-Z]',' ',sentence)\n",
    "      sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ',sentence)\n",
    "      sentence = re.sub(r'\\s+', ' ',sentence)\n",
    "\n",
    "      return sentence\n",
    "\n",
    "    temp=preprocess(txt)\n",
    "    sentList=nltk.sent_tokenize(temp)\n",
    "\n",
    "    #Word tokenize and Part-of-speech Tag each word\n",
    "\n",
    "    for line in sentList:\n",
    "        txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(txt_list)\n",
    "    print(taggedList)\n",
    "\n",
    "    sentList\n",
    "\n",
    "    #Join words with two nouns together(E.g. hair product = hairproduct)then pos tag everything again\n",
    "\n",
    "    newwordList = []\n",
    "    flag = 0\n",
    "    for i in range(0,len(taggedList)-1):\n",
    "        if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "            newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "            flag=1\n",
    "        else:\n",
    "            if(flag==1):\n",
    "                flag=0\n",
    "                continue\n",
    "            newwordList.append(taggedList[i][0])\n",
    "            if(i==len(taggedList)-2):\n",
    "                newwordList.append(taggedList[i+1][0])\n",
    "    finaltxt = ' '.join(word for word in newwordList)\n",
    "    print('final',finaltxt)\n",
    "\n",
    "    sentList = nltk.sent_tokenize(finaltxt)\n",
    "    for line in sentList:\n",
    "        txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(txt_list)\n",
    "    print(taggedList)\n",
    "\n",
    "    taggedList\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "    wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "    taggedList = nltk.pos_tag(wordsList)\n",
    "    \n",
    "    #exception for GW\n",
    "    for i in range(0,len(taggedList)):\n",
    "        if taggedList[i][1]=='GW':\n",
    "            new=(taggedList[i][0],'JJ')\n",
    "            taggedList[i] = new\n",
    "\n",
    "    doc = nlp(finaltxt)\n",
    "    dep_node = []\n",
    "    for dep_edge in doc.sentences[0].dependencies:\n",
    "        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "    for i in range(0, len(dep_node)):\n",
    "        if (int(dep_node[i][1]) != 0):\n",
    "            dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "    \n",
    "\n",
    "    (doc.sentences[0].dependencies)[1]\n",
    "\n",
    "    featureList = []\n",
    "    categories = []\n",
    "    totalfeatureList = []\n",
    "    print(taggedList)\n",
    "    for i in taggedList:\n",
    "        if(i[1]=='JJ' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "            featureList.append(list(i))\n",
    "            totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "            categories.append(i[0])\n",
    "    #print(featureList)\n",
    "    #print(totalfeatureList)\n",
    "    #print(categories)\n",
    "\n",
    "    fcluster = []\n",
    "    for i in featureList:\n",
    "        filist = []\n",
    "        for j in dep_node:\n",
    "            if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                if(j[0]==i[0]):\n",
    "                    filist.append(j[1])\n",
    "                else:\n",
    "                    filist.append(j[0])\n",
    "        fcluster.append([i[0], filist])\n",
    "    \n",
    "\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "    for i in featureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\" or dic[i[0]]==\"NNS\" or dic[i[0]]==\"NNP\" or dic[i[0]]==\"NNPS\"):\n",
    "            finalcluster.append(i)\n",
    "    print(finalcluster)\n",
    "    aspect.append(finalcluster)\n",
    "df['cluster']=aspect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
